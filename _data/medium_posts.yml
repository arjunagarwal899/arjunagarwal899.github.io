# Auto-generated from Medium RSS feed
posts:
  - title: "Understanding LoRA: Fine-Tuning Large Models Without Breaking the GPU"
    link: "https://medium.com/@arjunagarwal899/understanding-lora-fine-tuning-large-models-without-breaking-the-gpu-2ecd097192fa?source=rss-82954e624086------2"
    pubDate: "Sat, 18 Oct 2025 14:30:32 GMT"
    description: "Paper: LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS (https://arxiv.org/pdf/2106.09685)\nOct 2021, Microsoft\nGoal\nIn today’s world, there exist a lot of highly capable general purpose LLMs. However, most use-cases require specialized versions of these LLMs which is not an easy task. Keeping aside data curation and preparation, which in itself is quite demanding, current LLMs are multi-billion (and sometimes even multi-trillion) parameter models, and fine-tuning the model for training will require a costly high resource multi-node setup. The authors of this paper aim to..."
    thumbnail: "https://cdn-images-1.medium.com/max/361/1*N5vneaRPykhwdq2RBuh0Xw.png"
    author: "Arjun Agarwal"
  - title: "Understanding InternVL: Vision-Language Alignment through Cross-Attention Mechanisms"
    link: "https://medium.com/@arjunagarwal899/understanding-internvl-vision-language-alignment-through-cross-attention-mechanisms-4a29523098f4?source=rss-82954e624086------2"
    pubDate: "Tue, 14 Oct 2025 13:42:03 GMT"
    description: "Paper: InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (https://arxiv.org/pdf/2312.14238)\nJan 2024, OpenGVLab, China\nGoal\nWith the rapid growth in VLMs, the authors of this paper aim to create a VLM with a large vision model (6B) that is aligned with an equally sized LLM (8B).\nInstead of connecting the vision and language models using projection layers, the authors use cross attention and progressively train the network in order to reach better results.\nArchitecture\nVision model\nThe authors use a simple ViT model for the image..."
    thumbnail: "https://cdn-images-1.medium.com/max/688/1*gUknLGWfpMem_5FDpa277A.png"
    author: "Arjun Agarwal"
  - title: "Reinforcement Learning Part 4: Value Iteration and Policy Iteration"
    link: "https://medium.com/@arjunagarwal899/reinforcement-learning-part-4-value-iteration-and-policy-iteration-21457427f9a9?source=rss-82954e624086------2"
    pubDate: "Thu, 09 Oct 2025 06:57:37 GMT"
    description: "By now we are comfortable with the terminology used in RL, the Bellman equation, the Bellman Optimality equation (BOE), and how we can solve one iteration of the BOE mathematically. Those who want to read the previous parts can find the links to the articles at the bottom. Now we will formally define three algorithms that allow us to achieve optimal policies. These algorithms have the following requirements:\nThe state space and action space must be finite.The dynamics model must be known i.e. the transition probabilities for each set of inputs must be known.All three..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*QwK0niwb-RKs1SOoD8nx_Q.png"
    author: "Arjun Agarwal"
  - title: "Reinforcement Learning Part 3: The Bellman Optimality Equation and Optimal Policies"
    link: "https://medium.com/@arjunagarwal899/reinforcement-learning-part-3-the-bellman-optimality-equation-and-optimal-policies-1303456ce592?source=rss-82954e624086------2"
    pubDate: "Fri, 19 Sep 2025 16:45:30 GMT"
    description: "Those joining us directly at Part 3 should be familiar with the Bellman Equation and how it can be used to compare two policies. We have defined optimal policies and optimal state values. We also know how to calculate state values iteratively rather than calculating a computationally-heavy matrix inverse using a step known as Policy Evaluation. Those interested in previous parts can find the links at the bottom.\nThe Bellman Optimality Equation\nThe Bellman Optimality Equation (BOE) builds on the Bellman Equation and tries to express the state values that an agent can achieve..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*QwK0niwb-RKs1SOoD8nx_Q.png"
    author: "Arjun Agarwal"
  - title: "Reinforcement Learning Part 2: The Bellman Equation and its Importance"
    link: "https://medium.com/@arjunagarwal899/reinforcement-learning-part-2-the-bellman-equation-and-its-importance-d65e4d18a6eb?source=rss-82954e624086------2"
    pubDate: "Thu, 11 Sep 2025 15:07:44 GMT"
    description: "For those joining this series directly at Part 2, Part 1 contained a list of terminology that is commonly used in RL, all of which the reader should be familiar with. Part 1 is linked at the bottom of this post.\nHow do we know which policy is better?\nLet us say for any given RL problem, we have to find the “best” policy that the agent should follow. To find the best policy, we first need to define what makes a policy better than another one. Intuitively speaking, a policy is better if the agent receives higher rewards. Mathematically, state values provide us with a way to..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*QwK0niwb-RKs1SOoD8nx_Q.png"
    author: "Arjun Agarwal"
  - title: "Understanding MAISI: Synthetic CT Scan Generation with Medical AI Diffusion Models"
    link: "https://medium.com/@arjunagarwal899/understanding-maisi-synthetic-ct-scan-generation-with-medical-ai-diffusion-models-ead222c6da95?source=rss-82954e624086------2"
    pubDate: "Wed, 10 Sep 2025 08:34:53 GMT"
    description: "Paper: MAISI: Medical AI for Synthetic Imaging (https://arxiv.org/pdf/2409.11169)\nOct 2024, NVIDIA\nWith generative AI being all the craze right now, diffusion models emerged as a highly effective means to create high fidelity and high diversity images. While much of the research in this field is aimed at controlling the outputs of these models, a small section of research is devoted to generating radiological scans, with an even smaller section of research aimed at generating 3D radiological scans, specifically Computed Tomography (CT) scans.\nOne may question why I am even..."
    thumbnail: "https://cdn-images-1.medium.com/max/610/1*rFrWKE4Qfs-Dtcu_YTCFZg.png"
    author: "Arjun Agarwal"
  - title: "Reinforcement Learning Part 1: Basic Terminology and the Markov Decision Process"
    link: "https://medium.com/@arjunagarwal899/reinforcement-learning-part-1-basic-terminology-and-the-markov-decision-process-5efcb5c9a411?source=rss-82954e624086------2"
    pubDate: "Tue, 09 Sep 2025 07:35:54 GMT"
    description: "Reinforcement learning is a branch of AI that has recently gained renewed attention through cutting-edge research and experimentation. While traditionally considered a slow and unstable training paradigm, its role in advancing large language models and the broader pursuit of AGI has revealed new benefits, making RL a powerful strategy that every AI scientist should have in their toolkit.\nTerminology\nLet us begin this with the most basic terminology that we will encounter throughout this series:\nAgent: An entity that perceives its environment and takes actions autonomously..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*QwK0niwb-RKs1SOoD8nx_Q.png"
    author: "Arjun Agarwal"
  - title: "Understanding Classifier-Free Guidance: Improving Control in Diffusion Models Without Additional…"
    link: "https://medium.com/@arjunagarwal899/understanding-classifier-free-guidance-improving-control-in-diffusion-models-without-additional-84f9b12bacd1?source=rss-82954e624086------2"
    pubDate: "Mon, 08 Sep 2025 11:27:55 GMT"
    description: "Understanding Classifier-Free Guidance: Improving Control in Diffusion Models Without Additional Networks\nPaper: CLASSIFIER-FREE DIFFUSION GUIDANCE (https://arxiv.org/pdf/2207.12598)\nJul 2022, Google Research\nGoal\nThe authors of this paper set out to achieve the results of Classifier Guidance without the classifier, which is the main innovation presented in the latter. Avoiding the disadvantages of this classifier is the primary motivation behind this paper. Additionally, they are also able to do this with a single stage of training compared to the two networks required to..."
    thumbnail: "https://cdn-images-1.medium.com/max/854/1*yrOSD-GR_mdL4wOcjBt2xQ.png"
    author: "Arjun Agarwal"
  - title: "Understanding Classifier Guidance: Steering Diffusion Models with Gradient Signals"
    link: "https://medium.com/@arjunagarwal899/understanding-classifier-guidance-steering-diffusion-models-with-gradient-signals-627554fc739e?source=rss-82954e624086------2"
    pubDate: "Fri, 05 Sep 2025 05:50:13 GMT"
    description: "Paper: Diffusion Models Beat GANs on Image Synthesis (https://openreview.net/pdf?id=AAWuCvzaVt)\nJune 2021, OpenAI\nGoal\nIt is well known that while GANs have superior image fidelity, they lack in terms of output diversity. While Diffusion Models (DMs) are a noteworthy competitor in the domain of image synthesis, providing high diversity in their outputs, they lack controllability in their outputs. The authors of this paper express two reasons for this disparity: extensive research on GANs leading to better results, and the inherent capability of GANs to be able to trade..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*k8nUQcimciDBh9e40jTZIQ.png"
    author: "Arjun Agarwal"
  - title: "Understanding DETR: When Transformers Decide Bounding Boxes Are Just Another Sequence"
    link: "https://medium.com/@arjunagarwal899/understanding-detr-when-transformers-decide-bounding-boxes-are-just-another-sequence-4ebf2c8a8214?source=rss-82954e624086------2"
    pubDate: "Sun, 31 Aug 2025 13:17:36 GMT"
    description: "Paper: End-to-End Object Detection with Transformers (https://arxiv.org/pdf/2005.12872)\nMay 2020, Facebook AI\nDETR: Detection Transformer\nTraditionally, detection models use complex strategies that employ the use of CNNs along with additional non-DL AI algorithms in order to generate bounding boxes with classification for objects of interest. The authors of this paper broke this trend by targeting the same goal but by using a simple transformer-based architecture and a smart method to calculate the loss. The authors strip detection architectures of region proposal networks,..."
    thumbnail: "https://cdn-images-1.medium.com/max/486/1*jdlBlEtenNOYHHmdiISgXQ.png"
    author: "Arjun Agarwal"
  - title: "Understanding LLaVa 1.5: Improvements in Training Recipes and Benchmarks"
    link: "https://medium.com/@arjunagarwal899/understanding-llava-1-5-improvements-in-training-recipes-and-benchmarks-49e5d8d11702?source=rss-82954e624086------2"
    pubDate: "Mon, 18 Aug 2025 08:23:43 GMT"
    description: "Paper: Improved Baselines with Visual Instruction Tuning (https://arxiv.org/pdf/2310.03744)\nMay 2024, UW-Madison, Microsoft Research\nIn this paper, the authors build upon their previous paper “Visual Instruction Tuning”. If you want to know more about LLaVa 1.0 i.e. the first paper, have a look here.\nThis core material presented in this paper can be divided into the following sections:\nComparison with other modelsArchitectural changesDataset changesResultsComparison with other models\nThe paper mainly focuses on the following competitors when it compares then with LLaVA 1...."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*yACt1qPTIT6c-pUdCVtvLQ.png"
    author: "Arjun Agarwal"
  - title: "Glossary: A Reference to All My Articles"
    link: "https://medium.com/@arjunagarwal899/glossary-of-all-my-articles-a059188e966f?source=rss-82954e624086------2"
    pubDate: "Sat, 16 Aug 2025 13:07:53 GMT"
    description: "I wanted to provide a method for readers to be able to find any and all of my articles in one place with some sort of segregation, which I find hard to do with Medium; this is why I have created a sort of index below which lists all the articles based on some defining topic.\nResearch Papers\nDiffusion Models\nUnderstanding DiT: Transformer-Driven Diffusion for Image SynthesisUnderstanding ControlNet: A Way to Boss Around Your Diffusion ModelUnderstanding LDMs: Diffusion in Latent Space for Efficient Resource UtilizationUnderstanding Classifier Guidance: Steering Diffusion..."
    thumbnail: "https://cdn-images-1.medium.com/max/626/0*llQeHHGJ5OVDL7-a.jpg"
    author: "Arjun Agarwal"
  - title: "Understanding DiT: Transformer-Driven Diffusion for Image Synthesis"
    link: "https://medium.com/@arjunagarwal899/understanding-dit-transformer-driven-diffusion-for-image-synthesis-ea587bca69bb?source=rss-82954e624086------2"
    pubDate: "Sat, 26 Jul 2025 04:36:23 GMT"
    description: "Paper: Scalable Diffusion Models with Transformers (https://arxiv.org/pdf/2212.09748)\nDiT: Diffusion Transformer\nMar 2023, UCB, NYU\nThe authors of this paper had a single goal in mind: to show superior performance by transformers compared to conv UNets in the LDM diffusion process. Before this paper, very limited research existed which employed the use of transformers in generative vision modeling; this paper changed that by showing that the inductive bias inherent in U-Nets are not a requirement for diffusion models to work.\n(to understand diffusion models, please read the..."
    thumbnail: "https://cdn-images-1.medium.com/max/421/1*_vXImjYjJmfoI04lpM331Q.png"
    author: "Arjun Agarwal"
  - title: "Understanding LLaVa: Merging Visual Perception with Large Language Models"
    link: "https://medium.com/@arjunagarwal899/understanding-llava-merging-visual-perception-with-large-language-models-fa860bf81e90?source=rss-82954e624086------2"
    pubDate: "Thu, 24 Jul 2025 11:29:40 GMT"
    description: "Paper: Visual Instruction Tuning (https://arxiv.org/pdf/2304.08485)\nLLaVa: Large Language and Vision Assistant\nDec 2023, UW Madison, Microsoft, Columbia\nWhile there are a lot of models that have been made to follow instructions in text format, there is not a lot of research in miltimodal models where the instruction is followed based on the information present in the image. This paper aims to address this gap by generating a relevant dataset using powerful LLMs (such as GPT) and then creating a large multimodal model using pre-trained LLMs and ViTs that are then fine-tuned..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*GbxYqzudskk0KSpZksxZ5Q.png"
    author: "Arjun Agarwal"
  - title: "Understanding ControlNet: A Way to Boss Around Your Diffusion Model"
    link: "https://medium.com/@arjunagarwal899/understanding-controlnet-a-way-to-boss-around-your-diffusion-model-811039b891f6?source=rss-82954e624086------2"
    pubDate: "Tue, 22 Jul 2025 16:43:18 GMT"
    description: "Paper: Adding Conditional Control to Text-to-Image Diffusion Models (https://arxiv.org/pdf/2302.05543)\nGoal\nWhile umpteen models exist that allow image generation using texts, the outputs are usually different from what the user visualized in their mind, and it does take a bit of tweaking and prompt engineering to get a satisfactory image. ControlNet is a method that is introduced to add spatial conditioning to a pretrained text-to-image diffusion model that produces outputs that are much closer to the user’s imagination with minimal effort. This spatial conditioning can be..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*Xqy5PDuUDx9qxc7yS3aklQ.png"
    author: "Arjun Agarwal"
  - title: "Overview of all optimizers: Making Bad Choices Faster Since 1986"
    link: "https://medium.com/@arjunagarwal899/overview-of-all-optimizers-making-bad-choices-faster-since-1986-6717be503917?source=rss-82954e624086------2"
    pubDate: "Tue, 01 Jul 2025 12:00:48 GMT"
    description: "While PyTorch, Keras, and other similar libraries handle gradient graph calculation based on the forward pass, it is actually the optimizer’s job to update the weights of the model at every update step. There are several optimizers to choose from, each with their own advantages and disadvantages. We shall discuss the following optimizers here:\nStochastic Gradient Descent (SGD)\nSGD is, by far, the simplest and easiest optimizer to understand as it’s the first method of updating model weights that anybody can think of intuitively.\nIn SGD, the weights are updated with a value..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*Bu9TzhR95zAM5VUd7boIUg.jpeg"
    author: "Arjun Agarwal"
  - title: "Understanding PyTorch’s Distributed Communication Package: Powering all distributed model training"
    link: "https://medium.com/@arjunagarwal899/understanding-pytorchs-distributed-communication-package-powering-all-distributed-model-training-340d6b553faf?source=rss-82954e624086------2"
    pubDate: "Sun, 04 May 2025 15:16:48 GMT"
    description: "PyTorch provides a very powerful API to communicate between multiple accelerator (GPU, CPU, etc.) instances when performing distributed model training, leading to high efficiency and optimization.\nThis API is powered by one of multiple backends. Since most of today’s training occurs on GPUs, the NCCL backend is the most commonly used backend, especially when training LLMs which require InfiniBand for efficient training. The next most observed backend is Gloo, followed by MPI, which allow training on multiple CPUs. PyTorch takes care of interfacing with the backend for us,..."
    thumbnail: "https://cdn-images-1.medium.com/max/795/1*r0_wDl8X2IxlKhGPr-sdZw.jpeg"
    author: "Arjun Agarwal"
  - title: "Understanding InfoVAE: Rethinking the ELBO to Avoid Posterior Collapse"
    link: "https://medium.com/@arjunagarwal899/understanding-infovae-rethinking-the-elbo-to-avoid-posterior-collapse-3ed369ef0e7f?source=rss-82954e624086------2"
    pubDate: "Wed, 09 Apr 2025 10:38:08 GMT"
    description: "Paper: InfoVAE: Balancing Learning and Inference in Variational Autoencoders (https://arxiv.org/pdf/1706.02262)\nGoal: To modify the ELBO such that the opposing forces of the reconstruction loss and the KL divergence is decoupled while optimizing for generation.\nThe authors first start by giving a recap of VAEs. The math behind the vanilla VAE can be found here.\nThe authors then state that the ELBO can theoretically be maximized even with inaccurate estimated posteriors. They say that “good ELBO values do not imply accurate inference.” They go on to show prove this by math,..."
    thumbnail: "https://cdn-images-1.medium.com/max/668/1*ZA8wZJuoQ6OvQgGaHDZLQw.png"
    author: "Arjun Agarwal"
  - title: "Understanding VampPrior: Identifying More Expressive VAE Priors"
    link: "https://medium.com/@arjunagarwal899/understanding-vampprior-identifying-more-expressive-priors-eba0753324db?source=rss-82954e624086------2"
    pubDate: "Sun, 06 Apr 2025 16:30:24 GMT"
    description: "Paper: VAE with a VampPrior (https://arxiv.org/pdf/1705.07120)\nChoosing a simple standard normal distribution as a prior for a VAE model has been shown to lead to over-regularization of the model and have insufficient utilization of the available latent space. The authors of this paper identify a new method to estimate a prior much closer to the true posterior while training the VAE, addressing the issues of the vanilla VAE.\nThe math behind it\nA VAE aims to optimize for two opposing terms simultaneously: the negative log likelihood of our desired data distribution, and the..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*beDm9Jey-4DrCZ9EY83RWg.png"
    author: "Arjun Agarwal"
  - title: "Understanding SR3: Super-Resolution using Diffusion Models"
    link: "https://medium.com/@arjunagarwal899/understanding-sr3-super-resolution-using-diffusion-models-a5c7b915f102?source=rss-82954e624086------2"
    pubDate: "Tue, 01 Apr 2025 06:56:06 GMT"
    description: "Paper: Image Super-Resolution via Iterative Refinement (https://arxiv.org/pdf/2104.07636)\nIn this paper, the authors introduce a new method to perform super-resolution (SR) on low resolution (LR) images using diffusion models. The existing methods at that time involved a different set of algorithms (regressive, adversarial, flow-based, etc.) which had their own set of issues. This paper tries to solve for them with diffusion models and shows better results based on human perception of the model outputs.\nThe training process\n(to understand diffusion models, please read the..."
    thumbnail: "https://cdn-images-1.medium.com/max/852/1*Y2Lbh0zpFBMT15bjpuAIZg.png"
    author: "Arjun Agarwal"
  - title: "Understanding MultiResUNet: Multi-Resolution Pathways for (Medical) Image Segmentation"
    link: "https://medium.com/@arjunagarwal899/understanding-multiresunet-multi-resolution-pathways-for-medical-image-segmentation-f7294cd84a55?source=rss-82954e624086------2"
    pubDate: "Sat, 29 Mar 2025 11:43:51 GMT"
    description: "Paper: MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation (https://arxiv.org/pdf/1902.04049)\nIn this paper, the authors attempt to improve upon the classical U-Net architecture by identifying the aspects where it lacks and incorporating changes accordingly.\nModifying the UNet block\nInstead of the two 3x3 conv layers in each scale of the U-Net encoder and decoder (which is approximately equivalent to a single 5x5 conv layer), they use three different kernel sizes (3, 5, and 7), and concatenate the outputs to get a single feature..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*7ZisnJeqtCENQWwEFvIcFw.png"
    author: "Arjun Agarwal"
  - title: "Understanding LDMs: Diffusion in Latent Space for Efficient Resource Utilization"
    link: "https://medium.com/@arjunagarwal899/understanding-latent-diffusion-models-the-foundation-behind-stable-diffusions-revolution-in-ai-e1fd575a19e9?source=rss-82954e624086------2"
    pubDate: "Fri, 28 Mar 2025 18:04:19 GMT"
    description: "Paper: High-Resolution Image Synthesis with Latent Diffusion Models (https://arxiv.org/pdf/2112.10752)\nDiffusion models have provided us with a method to generate conditioned high quality samples from a model that trains in a stable manner, often achieving state-of-the-art results. But training diffusion models in the pixel-space can be costly in terms of time and resources. This paper aims to introduce latent diffusion models (LDM) that operate in the latent-space instead of the pixel-space to reduce the time and resouce dependencies drastically.\nThe two stages of training..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*o8xev4TzSK2-pL54u9BYZQ.png"
    author: "Arjun Agarwal"

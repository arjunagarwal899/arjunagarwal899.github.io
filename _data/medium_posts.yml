# Auto-generated from Medium RSS feed
posts:
  - title: "Understanding DiT: Transformer-Driven Diffusion for Image Synthesis"
    link: "https://medium.com/@arjunagarwal899/understanding-dit-transformer-driven-diffusion-for-image-synthesis-ea587bca69bb?source=rss-82954e624086------2"
    pubDate: "Sat, 26 Jul 2025 04:36:23 GMT"
    description: "Paper: Scalable Diffusion Models with Transformers (https://arxiv.org/pdf/2212.09748)\nDiT: Diffusion Transformer\nMar 2023, UCB, NYU\nThe authors of this paper had a single goal in mind: to show superior performance by transformers compared to conv UNets in the LDM diffusion process. Before this paper, very limited research existed which employed the use of transformers in generative vision modeling; this paper changed that by showing that the inductive bias inherent in U-Nets are not a requirement for diffusion models to work.\n(to understand diffusion models, please read the..."
    thumbnail: "https://cdn-images-1.medium.com/max/421/1*_vXImjYjJmfoI04lpM331Q.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding LLaVa: Merging Visual Perception with Large Language Models"
    link: "https://medium.com/@arjunagarwal899/understanding-llava-merging-visual-perception-with-large-language-models-fa860bf81e90?source=rss-82954e624086------2"
    pubDate: "Thu, 24 Jul 2025 11:29:40 GMT"
    description: "Paper: Visual Instruction Tuning (https://arxiv.org/pdf/2304.08485)\nLLaVa: Large Language and Vision Assistant\nDec 2023, UW Madison, Microsoft, Columbia\nWhile there are a lot of models that have been made to follow instructions in text format, there is not a lot of research in miltimodal models where the instruction is followed based on the information present in the image. This paper aims to address this gap by generating a relevant dataset using powerful LLMs (such as GPT) and then creating a large multimodal model using pre-trained LLMs and ViTs that are then fine-tuned..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*GbxYqzudskk0KSpZksxZ5Q.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding ControlNet: A Way to Boss Around Your Diffusion Model"
    link: "https://medium.com/@arjunagarwal899/understanding-controlnet-a-way-to-boss-around-your-diffusion-model-811039b891f6?source=rss-82954e624086------2"
    pubDate: "Tue, 22 Jul 2025 16:43:18 GMT"
    description: "Paper: Adding Conditional Control to Text-to-Image Diffusion Models (https://arxiv.org/pdf/2302.05543)\nGoal\nWhile umpteen models exist that allow image generation using texts, the outputs are usually different from what the user visualized in their mind, and it does take a bit of tweaking and prompt engineering to get a satisfactory image. ControlNet is a method that is introduced to add spatial conditioning to a pretrained text-to-image diffusion model that produces outputs that are much closer to the user’s imagination with minimal effort. This spatial conditioning can be..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*Xqy5PDuUDx9qxc7yS3aklQ.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Overview of all optimizers: Making Bad Choices Faster Since 1986"
    link: "https://medium.com/@arjunagarwal899/overview-of-all-optimizers-making-bad-choices-faster-since-1986-6717be503917?source=rss-82954e624086------2"
    pubDate: "Tue, 01 Jul 2025 12:00:48 GMT"
    description: "While PyTorch, Keras, and other similar libraries handle gradient graph calculation based on the forward pass, it is actually the optimizer’s job to update the weights of the model at every update step. There are several optimizers to choose from, each with their own advantages and disadvantages. We shall discuss the following optimizers here:\nStochastic Gradient Descent (SGD)\nSGD is, by far, the simplest and easiest optimizer to understand as it’s the first method of updating model weights that anybody can think of intuitively.\nIn SGD, the weights are updated with a value..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*Bu9TzhR95zAM5VUd7boIUg.jpeg"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding PyTorch’s Distributed Communication Package: Powering all distributed model training"
    link: "https://medium.com/@arjunagarwal899/understanding-pytorchs-distributed-communication-package-powering-all-distributed-model-training-340d6b553faf?source=rss-82954e624086------2"
    pubDate: "Sun, 04 May 2025 15:16:48 GMT"
    description: "PyTorch provides a very powerful API to communicate between multiple accelerator (GPU, CPU, etc.) instances when performing distributed model training, leading to high efficiency and optimization.\nThis API is powered by one of multiple backends. Since most of today’s training occurs on GPUs, the NCCL backend is the most commonly used backend, especially when training LLMs which require InfiniBand for efficient training. The next most observed backend is Gloo, followed by MPI, which allow training on multiple CPUs. PyTorch takes care of interfacing with the backend for us,..."
    thumbnail: "https://cdn-images-1.medium.com/max/795/1*r0_wDl8X2IxlKhGPr-sdZw.jpeg"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding InfoVAE: Rethinking the ELBO to Avoid Posterior Collapse"
    link: "https://medium.com/@arjunagarwal899/understanding-infovae-rethinking-the-elbo-to-avoid-posterior-collapse-3ed369ef0e7f?source=rss-82954e624086------2"
    pubDate: "Wed, 09 Apr 2025 10:38:08 GMT"
    description: "Paper: InfoVAE: Balancing Learning and Inference in Variational Autoencoders (https://arxiv.org/pdf/1706.02262)\nGoal: To modify the ELBO such that the opposing forces of the reconstruction loss and the KL divergence is decoupled while optimizing for generation.\nThe authors first start by giving a recap of VAEs. The math behind the vanilla VAE can be found here.\nThe authors then state that the ELBO can theoretically be maximized even with inaccurate estimated posteriors. They say that “good ELBO values do not imply accurate inference.” They go on to show prove this by math,..."
    thumbnail: "https://cdn-images-1.medium.com/max/668/1*ZA8wZJuoQ6OvQgGaHDZLQw.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding VampPrior: Identifying More Expressive VAE Priors"
    link: "https://medium.com/@arjunagarwal899/understanding-vampprior-identifying-more-expressive-priors-eba0753324db?source=rss-82954e624086------2"
    pubDate: "Sun, 06 Apr 2025 16:30:24 GMT"
    description: "Paper: VAE with a VampPrior (https://arxiv.org/pdf/1705.07120)\nChoosing a simple standard normal distribution as a prior for a VAE model has been shown to lead to over-regularization of the model and have insufficient utilization of the available latent space. The authors of this paper identify a new method to estimate a prior much closer to the true posterior while training the VAE, addressing the issues of the vanilla VAE.\nThe math behind it\nA VAE aims to optimize for two opposing terms simultaneously: the negative log likelihood of our desired data distribution, and the..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*beDm9Jey-4DrCZ9EY83RWg.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding SR3: Super-Resolution using Diffusion Models"
    link: "https://medium.com/@arjunagarwal899/understanding-sr3-super-resolution-using-diffusion-models-a5c7b915f102?source=rss-82954e624086------2"
    pubDate: "Tue, 01 Apr 2025 06:56:06 GMT"
    description: "Paper: Image Super-Resolution via Iterative Refinement (https://arxiv.org/pdf/2104.07636)\nIn this paper, the authors introduce a new method to perform super-resolution (SR) on low resolution (LR) images using diffusion models. The existing methods at that time involved a different set of algorithms (regressive, adversarial, flow-based, etc.) which had their own set of issues. This paper tries to solve for them with diffusion models and shows better results based on human perception of the model outputs.\nThe training process\n(to understand diffusion models, please read the..."
    thumbnail: "https://cdn-images-1.medium.com/max/852/1*Y2Lbh0zpFBMT15bjpuAIZg.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding MultiResUNet: Multi-Resolution Pathways for (Medical) Image Segmentation"
    link: "https://medium.com/@arjunagarwal899/understanding-multiresunet-multi-resolution-pathways-for-medical-image-segmentation-f7294cd84a55?source=rss-82954e624086------2"
    pubDate: "Sat, 29 Mar 2025 11:43:51 GMT"
    description: "Paper: MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation (https://arxiv.org/pdf/1902.04049)\nIn this paper, the authors attempt to improve upon the classical U-Net architecture by identifying the aspects where it lacks and incorporating changes accordingly.\nModifying the UNet block\nInstead of the two 3x3 conv layers in each scale of the U-Net encoder and decoder (which is approximately equivalent to a single 5x5 conv layer), they use three different kernel sizes (3, 5, and 7), and concatenate the outputs to get a single feature..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*7ZisnJeqtCENQWwEFvIcFw.png"
    categories: []
    author: "Arjun Agarwal"
  - title: "Understanding LDMs: Diffusion in Latent Space for Efficient Resource Utilization"
    link: "https://medium.com/@arjunagarwal899/understanding-latent-diffusion-models-the-foundation-behind-stable-diffusions-revolution-in-ai-e1fd575a19e9?source=rss-82954e624086------2"
    pubDate: "Fri, 28 Mar 2025 18:04:19 GMT"
    description: "Paper: High-Resolution Image Synthesis with Latent Diffusion Models (https://arxiv.org/pdf/2112.10752)\nDiffusion models have provided us with a method to generate conditioned high quality samples from a model that trains in a stable manner, often achieving state-of-the-art results. But training diffusion models in the pixel-space can be costly in terms of time and resources. This paper aims to introduce latent diffusion models (LDM) that operate in the latent-space instead of the pixel-space to reduce the time and resouce dependencies drastically.\nThe two stages of training..."
    thumbnail: "https://cdn-images-1.medium.com/max/1024/1*o8xev4TzSK2-pL54u9BYZQ.png"
    categories: []
    author: "Arjun Agarwal"
